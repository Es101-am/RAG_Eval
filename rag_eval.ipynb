{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0035cdba",
   "metadata": {},
   "source": [
    "*********Leveraging RAGAS and Advanced Retrieval Techniques with LangChain**********"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47877172",
   "metadata": {},
   "source": [
    "*****Load Docs****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "943cbd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -q langchain openai ragas pymupdf chromadb wandb tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "301f72d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q beautifulsoup4\n",
    "!pip install -q lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f789cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "openai.api_key = \"\" \n",
    "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "426327e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Source URLs\n",
    "urls = [\n",
    "    \"https://www.pinecone.io/learn/retrieval-augmented-generation/\",\n",
    "    \"https://docs.llamaindex.ai/en/stable/getting_started/concepts.html\",\n",
    "    \"https://www.anyscale.com/blog/retrieval-augmented-generation-with-ray-and-hugging-face\",\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"\n",
    "]\n",
    "\n",
    "# Load documents\n",
    "docs = []\n",
    "for url in urls:\n",
    "    docs.extend(WebBaseLoader(url).load())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce58d6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://www.pinecone.io/learn/retrieval-augmented-generation/', 'title': 'Retrieval-Augmented Generation (RAG) | Pinecone', 'description': 'Explore the limitations of foundation models and how retrieval-augmented generation (RAG) can address these limitations so chat, search, and agentic workflows can all benefit.', 'language': 'en'}\n",
      "{'source': 'https://docs.llamaindex.ai/en/stable/getting_started/concepts.html', 'title': 'Redirecting...', 'language': 'en'}\n",
      "{'source': 'https://www.anyscale.com/blog/retrieval-augmented-generation-with-ray-and-hugging-face', 'title': 'Blog | Anyscale', 'description': 'Powered by Ray, Anyscale empowers AI builders to run and scale all ML and AI workloads on any cloud and on-prem.', 'language': 'en-US'}\n",
      "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}\n",
      "{'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}\n",
      "{'source': 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/', 'title': \"Adversarial Attacks on LLMs | Lil'Log\", 'description': 'The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output something undesired.\\nA large body of ground work on adversarial attacks is on images, and differently it operates in the continuous, high-dimensional space. Attacks for discrete data like text have been considered to be a lot more challenging, due to lack of direct gradient signals. My past post on Controllable Text Generation is quite relevant to this topic, as attacking LLMs is essentially to control the model to output a certain type of (unsafe) content.', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc2a65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pysqlite3-binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "407e0fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U \"langchain-community>=0.2.10\" \"langchain-openai>=0.1.7\" \"chromadb>=0.4.22,<0.5\" pysqlite3-binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf6047af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: chromadb 0.4.24\n",
      "Uninstalling chromadb-0.4.24:\n",
      "  Successfully uninstalled chromadb-0.4.24\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y chromadb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b037656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q chromadb==0.4.24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5e6422f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 0.4.24\n"
     ]
    }
   ],
   "source": [
    "!pip show chromadb | grep Version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06c05986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "try:\n",
    "    import pysqlite3\n",
    "    sys.modules[\"sqlite3\"] = pysqlite3\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Assume `base_docs` is a list[Document]\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    split_docs,\n",
    "    OpenAIEmbeddings(),\n",
    "    persist_directory=\"./chroma_store\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f67658e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462\n"
     ]
    }
   ],
   "source": [
    "print(len(split_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8018bd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n"
     ]
    }
   ],
   "source": [
    "print(max(len(chunk.page_content) for chunk in split_docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b8276d",
   "metadata": {},
   "source": [
    "****Basic QA Chain***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48945d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U langchain-openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76d42134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Initialize the primary LLM for Q&A\n",
    "qa_model = ChatOpenAI(model_name=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "\n",
    "# Create a RetrievalQA chain using the vector store retriever\n",
    "qa_pipeline = RetrievalQA.from_chain_type(\n",
    "    llm=qa_model,\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4c8bc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5037/4285062893.py:5: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = qa_pipeline({\"query\": user_query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Augmented Generation (RAG) is a technique that uses authoritative, external data to enhance the accuracy, relevancy, and usefulness of a model's output. It involves ingesting authoritative data into a data source, retrieving relevant data from external sources, and using this data to improve the generation process.\n"
     ]
    }
   ],
   "source": [
    "# Example user question\n",
    "user_query = \"What is RAG?\"\n",
    "\n",
    "# Run the question through the QA pipeline\n",
    "answer = qa_pipeline({\"query\": user_query})\n",
    "\n",
    "# Display only the answer text\n",
    "print(answer[\"result\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8213d52",
   "metadata": {},
   "source": [
    "***RAG***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac23e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "# Define the schema for extracting a question from the context\n",
    "schema_question = ResponseSchema(\n",
    "    name=\"question\",\n",
    "    description=\"Formulated question based on the given context.\"\n",
    ")\n",
    "\n",
    "# List of response schemas to be used by the output parser\n",
    "response_schemas = [schema_question]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcaa32f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output parser from the defined response schemas\n",
    "question_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Retrieve the formatting instructions for the parser\n",
    "format_instructions = question_parser.get_format_instructions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c42b9d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cf60134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5037/4103957451.py:26: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  model_output = llm_model(message_batch)\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI  # updated import\n",
    "# Define a prompt template for generating questions\n",
    "question_prompt_text = \"\"\"\n",
    "You are a specialist in preparing challenging questions for advanced learners.\n",
    "Given a context, formulate a relevant question.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_builder = ChatPromptTemplate.from_template(question_prompt_text)\n",
    "\n",
    "message_batch = prompt_builder.format_messages(\n",
    "    context=texts[0],\n",
    "    format_instructions=format_instructions,  # from question_parser.get_format_instructions()\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the LLM\n",
    "llm_model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Generate the response\n",
    "model_output = llm_model(message_batch)\n",
    "\n",
    "# Parse the structured response\n",
    "parsed_question = question_parser.parse(model_output.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efdeb44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:\n",
      "What are the limitations of foundation models and how can retrieval-augmented generation (RAG) address these limitations to benefit chat, search, and agentic workflows?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in parsed_question.items():\n",
    "    print(f\"{key}:\\n{value}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18cb1bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03dd98f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1c0bed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 462/462 [06:20<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "qa_context_pairs = []\n",
    "\n",
    "for chunk in tqdm(texts, desc=\"Generating questions\"):\n",
    "    # Prepare the LLM input\n",
    "    prompt_msgs = prompt_builder.format_messages(\n",
    "        context=chunk,\n",
    "        format_instructions=format_instructions\n",
    "    )\n",
    "    \n",
    "    # Get LLM output\n",
    "    llm_reply = llm_model(prompt_msgs)\n",
    "    \n",
    "    try:\n",
    "        parsed_output = question_parser.parse(llm_reply.content)\n",
    "    except Exception:\n",
    "        # Ignore invalid responses\n",
    "        continue\n",
    "    \n",
    "    # Attach the original context\n",
    "    parsed_output[\"context\"] = chunk\n",
    "    qa_context_pairs.append(parsed_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20b62a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_openai import ChatOpenAI  # updated import for latest LangChain\n",
    "\n",
    "# Initialize the LLM for answer generation\n",
    "answer_llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "# Create the schema for extracting the answer field\n",
    "schema_answer = ResponseSchema(\n",
    "    name=\"answer\",\n",
    "    description=\"The generated answer for the given question.\"\n",
    ")\n",
    "\n",
    "# Bundle schemas in a list (can add more if needed)\n",
    "response_schemas = [schema_answer]\n",
    "\n",
    "# Build the parser from the schema(s)\n",
    "answer_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Get parser-specific formatting instructions\n",
    "format_instructions = answer_parser.get_format_instructions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2ec5e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Template for generating answers from questions and context\n",
    "answer_prompt_text = \"\"\"\n",
    "You are a domain expert. Given a question and its related context, provide an accurate answer.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "\"\"\"\n",
    "\n",
    "# Build the prompt template\n",
    "answer_prompt = ChatPromptTemplate.from_template(template=answer_prompt_text)\n",
    "\n",
    "# Prepare the prompt for the first QAC triple\n",
    "answer_messages = answer_prompt.format_messages(\n",
    "    context=qa_context_pairs[0][\"context\"],\n",
    "    question=qa_context_pairs[0][\"question\"],\n",
    "    format_instructions=format_instructions\n",
    ")\n",
    "\n",
    "# Get the model's response\n",
    "answer_response = answer_llm(answer_messages)\n",
    "\n",
    "# Parse the structured output\n",
    "parsed_answer = answer_parser.parse(answer_response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "881593ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer\n",
      "The context does not provide specific details on the limitations of foundation models or how Retrieval-Augmented Generation (RAG) can address these limitations.\n"
     ]
    }
   ],
   "source": [
    "for k, v in parsed_answer.items():\n",
    "    print(k)\n",
    "    print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3856c0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 449/449 [23:11<00:00,  3.10s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Loop through each question-context pair and generate an answer\n",
    "for entry in tqdm(qa_context_pairs, desc=\"Generating answers\"):\n",
    "    prompt_msgs = answer_prompt.format_messages(\n",
    "        context=entry[\"context\"],\n",
    "        question=entry[\"question\"],\n",
    "        format_instructions=format_instructions\n",
    "    )\n",
    "\n",
    "    model_reply = answer_llm(prompt_msgs)\n",
    "\n",
    "    try:\n",
    "        parsed_result = answer_parser.parse(model_reply.content)\n",
    "    except Exception:\n",
    "        continue  # Skip if parsing fails\n",
    "\n",
    "    # Store the generated answer back into the dictionary\n",
    "    entry[\"answer\"] = parsed_result[\"answer\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33459377",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U datasets pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11c4c051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 76.95ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "925866"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Create a DataFrame from the question-context-answer records\n",
    "df_ground_truth = pd.DataFrame(qa_context_pairs)\n",
    "\n",
    "# Make sure context values are stored as strings\n",
    "df_ground_truth[\"context\"] = df_ground_truth[\"context\"].astype(str)\n",
    "\n",
    "# Rename the answer column to match evaluation naming\n",
    "df_ground_truth = df_ground_truth.rename(columns={\"answer\": \"ground_truth\"})\n",
    "\n",
    "# Convert to a Hugging Face Dataset\n",
    "eval_dataset = Dataset.from_pandas(df_ground_truth)\n",
    "\n",
    "# Save to CSV for later evaluation\n",
    "eval_dataset.to_csv(\"groundtruth_eval_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6afe9e9",
   "metadata": {},
   "source": [
    "**************RAG Evaluation****************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7029049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load from CSV into pandas\n",
    "df_eval = pd.read_csv(\"groundtruth_eval_dataset.csv\")\n",
    "\n",
    "# Convert back to Hugging Face Dataset\n",
    "eval_dataset = Dataset.from_pandas(df_eval)\n",
    "\n",
    "# (Optional) Keep only first 10 rows while preserving Dataset type\n",
    "eval_dataset = eval_dataset.select(range(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "591495d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U Pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "431e95e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: ragas 0.3.0\n",
      "Uninstalling ragas-0.3.0:\n",
      "  Successfully uninstalled ragas-0.3.0\n",
      "Collecting git+https://github.com/explodinggradients/ragas.git\n",
      "  Cloning https://github.com/explodinggradients/ragas.git to /tmp/pip-req-build-nksvgahl\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/explodinggradients/ragas.git /tmp/pip-req-build-nksvgahl\n",
      "  Resolved https://github.com/explodinggradients/ragas.git to commit 701d66c54031e41a443bf9046c43d296ceefda27\n",
      "\u001b[31mERROR: git+https://github.com/explodinggradients/ragas.git does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y ragas\n",
    "!pip install git+https://github.com/explodinggradients/ragas.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c892f6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ragas==0.3.0\n",
      "  Using cached ragas-0.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from ragas==0.3.0) (2.2.6)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.10/site-packages (from ragas==0.3.0) (4.0.0)\n",
      "Requirement already satisfied: tiktoken in ./.venv/lib/python3.10/site-packages (from ragas==0.3.0) (0.10.0)\n",
      "Requirement already satisfied: langchain in ./.venv/lib/python3.10/site-packages (from ragas==0.3.0) (0.3.27)\n",
      "Requirement already satisfied: langchain-core in ./.venv/lib/python3.10/site-packages (from ragas==0.3.0) (0.3.74)\n",
      "Requirement already satisfied: langchain-community in ./.venv/lib/python3.10/site-packages (from ragas==0.3.0) (0.3.27)\n",
      "Requirement already satisfied: langchain_openai in ./.venv/lib/python3.10/site-packages (from ragas==0.3.0) (0.3.28)\n",
      "Requirement already satisfied: nest-asyncio in ./.venv/lib/python3.10/site-packages (from ragas==0.3.0) (1.6.0)\n",
      "Requirement already satisfied: appdirs in ./.venv/lib/python3.10/site-packages (from ragas==0.3.0) (1.4.4)\n",
      "Requirement already satisfied: pydantic>=2 in ./.venv/lib/python3.10/site-packages (from ragas==0.3.0) (2.11.7)\n",
      "Requirement already satisfied: openai>1 in ./.venv/lib/python3.10/site-packages (from ragas==0.3.0) (1.99.3)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in ./.venv/lib/python3.10/site-packages (from ragas==0.3.0) (5.6.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.10/site-packages (from openai>1->ragas==0.3.0) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.10/site-packages (from openai>1->ragas==0.3.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from openai>1->ragas==0.3.0) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from openai>1->ragas==0.3.0) (0.10.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.10/site-packages (from openai>1->ragas==0.3.0) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.10/site-packages (from openai>1->ragas==0.3.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.venv/lib/python3.10/site-packages (from openai>1->ragas==0.3.0) (4.14.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>1->ragas==0.3.0) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>1->ragas==0.3.0) (3.10)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>1->ragas==0.3.0) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>1->ragas==0.3.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas==0.3.0) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.10/site-packages (from pydantic>=2->ragas==0.3.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.10/site-packages (from pydantic>=2->ragas==0.3.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic>=2->ragas==0.3.0) (0.4.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from datasets->ragas==0.3.0) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.10/site-packages (from datasets->ragas==0.3.0) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from datasets->ragas==0.3.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from datasets->ragas==0.3.0) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.10/site-packages (from datasets->ragas==0.3.0) (2.32.4)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.10/site-packages (from datasets->ragas==0.3.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.10/site-packages (from datasets->ragas==0.3.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./.venv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ragas==0.3.0) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.10/site-packages (from datasets->ragas==0.3.0) (0.34.4)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from datasets->ragas==0.3.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from datasets->ragas==0.3.0) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ragas==0.3.0) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ragas==0.3.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ragas==0.3.0) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ragas==0.3.0) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ragas==0.3.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ragas==0.3.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ragas==0.3.0) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ragas==0.3.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ragas==0.3.0) (1.20.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets->ragas==0.3.0) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets->ragas==0.3.0) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets->ragas==0.3.0) (2.5.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in ./.venv/lib/python3.10/site-packages (from langchain->ragas==0.3.0) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in ./.venv/lib/python3.10/site-packages (from langchain->ragas==0.3.0) (0.4.13)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.venv/lib/python3.10/site-packages (from langchain->ragas==0.3.0) (2.0.42)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.10/site-packages (from langchain-core->ragas==0.3.0) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.10/site-packages (from langchain-core->ragas==0.3.0) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas==0.3.0) (3.0.0)\n",
      "Requirement already satisfied: greenlet>=1 in ./.venv/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas==0.3.0) (3.2.4)\n",
      "Requirement already satisfied: orjson>=3.9.14 in ./.venv/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain->ragas==0.3.0) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./.venv/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain->ragas==0.3.0) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./.venv/lib/python3.10/site-packages (from langsmith>=0.1.17->langchain->ragas==0.3.0) (0.23.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./.venv/lib/python3.10/site-packages (from langchain-community->ragas==0.3.0) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in ./.venv/lib/python3.10/site-packages (from langchain-community->ragas==0.3.0) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from langchain-community->ragas==0.3.0) (0.4.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas==0.3.0) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./.venv/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas==0.3.0) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./.venv/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas==0.3.0) (1.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./.venv/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas==0.3.0) (1.1.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.10/site-packages (from tiktoken->ragas==0.3.0) (2025.7.34)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->datasets->ragas==0.3.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets->ragas==0.3.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas->datasets->ragas==0.3.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas==0.3.0) (1.17.0)\n",
      "Using cached ragas-0.3.0-py3-none-any.whl (190 kB)\n",
      "Installing collected packages: ragas\n",
      "Successfully installed ragas-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ragas==0.3.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a36757b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "# from ragas.metrics.critique import harmfulness\n",
    "from ragas import evaluate\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# def build_ragas_dataset(rag_pipeline, eval_data):\n",
    "#     \"\"\"Generate a RAGAS-compatible dataset from the given evaluation records.\"\"\"\n",
    "#     generated_entries = []\n",
    "    \n",
    "#     for record in tqdm(eval_data, desc=\"Running RAG pipeline\"):\n",
    "#         # Get the pipeline output for the given question\n",
    "#         pipeline_result = rag_pipeline({\"query\": record[\"question\"]})\n",
    "        \n",
    "#         # Collect structured data for RAGAS evaluation\n",
    "#         generated_entries.append({\n",
    "#             \"question\": record[\"question\"],\n",
    "#             \"answer\": pipeline_result[\"result\"],\n",
    "#             \"contexts\": [doc.page_content for doc in pipeline_result[\"source_documents\"]],\n",
    "#             \"ground_truths\": [record[\"ground_truth\"]]\n",
    "#         })\n",
    "    \n",
    "#     # Convert to Hugging Face Dataset\n",
    "#     df_ragas = pd.DataFrame(generated_entries)\n",
    "#     return Dataset.from_pandas(df_ragas)\n",
    "\n",
    "\n",
    "def build_ragas_dataset(rag_pipeline, eval_data):\n",
    "    from collections.abc import Mapping\n",
    "    import pandas as pd\n",
    "    from datasets import Dataset\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Normalize input â†’ list[dict]\n",
    "    if isinstance(eval_data, pd.DataFrame):\n",
    "        rows = eval_data.to_dict(orient=\"records\")\n",
    "    elif isinstance(eval_data, Dataset):\n",
    "        rows = list(eval_data)\n",
    "    elif isinstance(eval_data, list) and (len(eval_data) == 0 or isinstance(eval_data[0], Mapping)):\n",
    "        rows = eval_data\n",
    "    else:\n",
    "        raise TypeError(\"eval_data must be DataFrame, Dataset, or list of dicts.\")\n",
    "\n",
    "    entries = []\n",
    "    for record in tqdm(rows, desc=\"Running RAG pipeline\"):\n",
    "        q = record[\"question\"]\n",
    "        gt = record[\"ground_truth\"]\n",
    "\n",
    "        out = rag_pipeline({\"query\": q})\n",
    "        entries.append({\n",
    "            \"question\": q,\n",
    "            \"answer\": out.get(\"result\", \"\"),\n",
    "            \"contexts\": [doc.page_content for doc in out.get(\"source_documents\", [])],  # list[str]\n",
    "            \"reference\": str(gt),  # <-- plain string, not list\n",
    "        })\n",
    "\n",
    "    return Dataset.from_pandas(pd.DataFrame(entries))\n",
    "\n",
    "\n",
    "\n",
    "def run_ragas_evaluation(ragas_dataset):\n",
    "    \"\"\"Evaluate a RAGAS dataset with selected metrics.\"\"\"\n",
    "    return evaluate(\n",
    "        ragas_dataset,\n",
    "        metrics=[\n",
    "            context_precision,\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_recall,\n",
    "        ],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5de92e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running RAG pipeline: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:16<00:00,  1.65s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Build the evaluation dataset for RAGAS\n",
    "basic_qa_ragas_dataset = build_ragas_dataset(qa_pipeline, eval_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ac96c3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'contexts', 'reference'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_qa_ragas_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "afc15f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_qa_ragas_dataset.to_pandas().to_csv(\"basic_qa_ragas_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "31a91700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40/40 [00:33<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "basic_qa_result = run_ragas_evaluation(basic_qa_ragas_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a335b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_precision': 0.8000, 'faithfulness': 0.7802, 'answer_relevancy': 0.9521, 'context_recall': 0.9083}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_qa_result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d237bc18",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1003492158.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[61], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    find . -name \"*.ipynb\" -type f -print0 | xargs -0 -I{} jupyter nbconvert --to notebook --ClearOutputPreprocessor.enabled=True --inplace \"{}\"\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Option A: strip outputs in-place (safe + simple)\n",
    "!pip install nbconvert\n",
    "find . -name \"*.ipynb\" -type f -print0 | xargs -0 -I{} jupyter nbconvert --to notebook --ClearOutputPreprocessor.enabled=True --inplace \"{}\"\n",
    "\n",
    "# (Optional) enforce stripping on every commit\n",
    "!pip install nbstripout\n",
    "nbstripout --install\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
